{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3294019d-3c91-4a41-b0b6-2a78d0dd144e",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-07-17T06:37:56.354653Z",
     "iopub.status.busy": "2023-07-17T06:37:56.354252Z",
     "iopub.status.idle": "2023-07-17T06:37:56.358375Z",
     "shell.execute_reply": "2023-07-17T06:37:56.357832Z",
     "shell.execute_reply.started": "2023-07-17T06:37:56.354629Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    HfArgumentParser\n",
    ")\n",
    "from train_lora import FinetuneArguments, LoraArguments, LoraTrainer\n",
    "from loguru import logger\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import numpy as np\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddf9061-3083-41fa-b695-203fa885fdf1",
   "metadata": {},
   "source": [
    "# prepare arguments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58060bc2-c49e-4771-8aef-e9672208b641",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T06:26:58.624295Z",
     "iopub.status.busy": "2023-07-17T06:26:58.623614Z",
     "iopub.status.idle": "2023-07-17T06:26:58.629453Z",
     "shell.execute_reply": "2023-07-17T06:26:58.628881Z",
     "shell.execute_reply.started": "2023-07-17T06:26:58.624268Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-07-17 06:26:58.626\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mcurrent_finetune_args--->FinetuneArguments(dataset_path='data/Med_datasets.jsonl', model_path='THUDM/chatglm2-6b', label_pad_token_id=-100, load_in_4bit=True, bnb_4bit_quant_type='nf4', bnb_4bit_compute_dtype='float32', seed=42, resume_from_checkpoint=None, final_model_path='Lora_Adapter_THUDM_chatglm2-6b/finally_adapter')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "finetune_args = HfArgumentParser(FinetuneArguments)\n",
    "finetune_args, = finetune_args.parse_json_file(json_file='args_file/finetune_args.json')\n",
    "logger.info(f\"current_finetune_args--->{finetune_args}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3b397bd-de4a-4639-8bbf-afee4431d5d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T06:27:08.335395Z",
     "iopub.status.busy": "2023-07-17T06:27:08.334800Z",
     "iopub.status.idle": "2023-07-17T06:27:08.345883Z",
     "shell.execute_reply": "2023-07-17T06:27:08.345297Z",
     "shell.execute_reply.started": "2023-07-17T06:27:08.335368Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-07-17 06:27:08.342\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mcurrent_train_args--->TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=100,\n",
      "evaluation_strategy=IntervalStrategy.STEPS,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=info,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=Lora_Adapter_THUDM_chatglm2-6b/runs/Jul17_06-27-08_dsw-22712-59c465dd8d-f2ws4,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "output_dir=Lora_Adapter_THUDM_chatglm2-6b,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=True,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=Lora_Adapter_THUDM_chatglm2-6b,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      "xpu_backend=None,\n",
      ")\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_args = HfArgumentParser(TrainingArguments)\n",
    "train_args, = train_args.parse_json_file(json_file='args_file/train_args.json')\n",
    "logger.info(f\"current_train_args--->{train_args}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45a001ab-7caf-41d6-9cac-211c034b54e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T06:27:20.672573Z",
     "iopub.status.busy": "2023-07-17T06:27:20.672001Z",
     "iopub.status.idle": "2023-07-17T06:27:20.677777Z",
     "shell.execute_reply": "2023-07-17T06:27:20.677102Z",
     "shell.execute_reply.started": "2023-07-17T06:27:20.672545Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-07-17 06:27:20.675\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mcurrent_lora_args--->LoraArguments(target_modules=['query_key_value'], r=12, lora_alpha=24, lora_dropout=0.05, bias='none', inference_mode=False, layers_to_transform=None, layers_pattern=None, Adapter_name='LoraAdapter')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "lora_args = HfArgumentParser(LoraArguments)\n",
    "lora_args, = lora_args.parse_json_file(json_file='args_file/lora_args.json')\n",
    "logger.info(f\"current_lora_args--->{lora_args}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f00de51-c171-4aa5-bb3c-8fa30e3e0301",
   "metadata": {},
   "source": [
    "# process datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2561a536-a1bc-418c-befe-00e5fcd5d48d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T06:28:03.819197Z",
     "iopub.status.busy": "2023-07-17T06:28:03.818503Z",
     "iopub.status.idle": "2023-07-17T06:28:03.844534Z",
     "shell.execute_reply": "2023-07-17T06:28:03.844008Z",
     "shell.execute_reply.started": "2023-07-17T06:28:03.819167Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-07-17 06:28:03.820\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[1mfrom data/Med_datasets.jsonl loading datasets and tokenize datasets\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"from {finetune_args.dataset_path} loading datasets and tokenize datasets\")\n",
    "datasets = load_from_disk(dataset_path=finetune_args.dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac112036-569e-4a9c-bb3f-6aa2b3e1d437",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T06:28:15.184330Z",
     "iopub.status.busy": "2023-07-17T06:28:15.183897Z",
     "iopub.status.idle": "2023-07-17T06:28:16.504521Z",
     "shell.execute_reply": "2023-07-17T06:28:16.503823Z",
     "shell.execute_reply.started": "2023-07-17T06:28:15.184302Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(finetune_args.model_path,\n",
    "                                          trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "110d88b2-961a-446b-a86d-95558449733d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T06:28:30.087840Z",
     "iopub.status.busy": "2023-07-17T06:28:30.087451Z",
     "iopub.status.idle": "2023-07-17T06:28:30.092117Z",
     "shell.execute_reply": "2023-07-17T06:28:30.091547Z",
     "shell.execute_reply.started": "2023-07-17T06:28:30.087816Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_dataset(data, label_pad_token_id=finetune_args.label_pad_token_id):\n",
    "    instruction = data['context']\n",
    "    target = data['target']\n",
    "    instruction_ids = tokenizer.encode(instruction, add_special_tokens=True)\n",
    "    target_ids = tokenizer.encode(target, add_special_tokens=False)\n",
    "    input_ids = instruction_ids + target_ids + [tokenizer.eos_token_id]\n",
    "    labels = ([label_pad_token_id] * len(instruction_ids) + target_ids\n",
    "              + [tokenizer.eos_token_id])\n",
    "    return {'input_ids': input_ids, 'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dfc3f4e-2d3a-4bb0-9fd3-35a3a4261ae4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T06:28:44.328811Z",
     "iopub.status.busy": "2023-07-17T06:28:44.328378Z",
     "iopub.status.idle": "2023-07-17T06:28:48.002551Z",
     "shell.execute_reply": "2023-07-17T06:28:48.001769Z",
     "shell.execute_reply.started": "2023-07-17T06:28:44.328785Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "remove_columns = datasets['train'].column_names\n",
    "datasets_train = datasets['train'].map(tokenize_dataset,\n",
    "                                       remove_columns=remove_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af29e8c8-1132-4667-bae9-e5c66984c070",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T06:28:57.603324Z",
     "iopub.status.busy": "2023-07-17T06:28:57.602939Z",
     "iopub.status.idle": "2023-07-17T06:28:58.165753Z",
     "shell.execute_reply": "2023-07-17T06:28:58.165163Z",
     "shell.execute_reply.started": "2023-07-17T06:28:57.603299Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    }
   ],
   "source": [
    "remove_columns = datasets['valid'].column_names\n",
    "datasets_valid = datasets['valid'].map(tokenize_dataset,\n",
    "                                       remove_columns=remove_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "238f5c02-1492-4600-811e-3ba3ca404174",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T06:29:08.138024Z",
     "iopub.status.busy": "2023-07-17T06:29:08.137626Z",
     "iopub.status.idle": "2023-07-17T06:29:08.145352Z",
     "shell.execute_reply": "2023-07-17T06:29:08.144751Z",
     "shell.execute_reply.started": "2023-07-17T06:29:08.138000Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-07-17 06:29:08.142\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[1mdatasets_train--->{'input_ids': [64790, 64792, 30910, 39501, 31211, 33182, 32103, 32834, 31930, 32184, 31123, 55073, 31793, 43021, 32834, 34301, 31788, 31123, 37881, 36266, 54530, 33287, 31123, 38545, 32548, 54567, 33287, 43082, 31123, 54535, 55124, 55643, 55216, 55802, 54820, 31155, 13, 31639, 31211, 31623, 32016, 31699, 32044, 35531, 45331, 31755, 37079, 54706, 31123, 32082, 32066, 31751, 35531, 45331, 55251, 57252, 31123, 31755, 34529, 57194, 54557, 54679, 31123, 31843, 35531, 43114, 31123, 34802, 32066, 54538, 31017, 54831, 54900, 32066, 31751, 36082, 55021, 54629, 31123, 54695, 42693, 31831, 31642, 32222, 31514, 13, 33287, 30954, 30910, 47383, 35531, 45331, 55251, 57252, 31201, 57194, 54557, 54679, 31201, 35531, 43114, 41641, 31123, 31975, 31017, 54831, 54900, 32066, 31951, 31123, 51634, 54541, 35531, 44234, 31155, 2], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 47383, 35531, 45331, 55251, 57252, 31201, 57194, 54557, 54679, 31201, 35531, 43114, 41641, 31123, 31975, 31017, 54831, 54900, 32066, 31951, 31123, 51634, 54541, 35531, 44234, 31155, 2]}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"datasets_train--->{datasets_train.select(range(2))[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d27c258-42d8-43be-8035-3984ac2111b2",
   "metadata": {},
   "source": [
    "# load model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7350dde3-6220-49ba-9f2e-08c6cfec33f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T06:29:49.551051Z",
     "iopub.status.busy": "2023-07-17T06:29:49.550346Z",
     "iopub.status.idle": "2023-07-17T06:31:07.069653Z",
     "shell.execute_reply": "2023-07-17T06:31:07.068810Z",
     "shell.execute_reply.started": "2023-07-17T06:29:49.551022Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-07-17 06:29:49.552\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[1mfrom THUDM/chatglm2-6b loading model\u001b[0m\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [01:12<00:00, 10.30s/it]\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"from {finetune_args.model_path} loading model\")\n",
    "model = AutoModel.from_pretrained(finetune_args.model_path,\n",
    "                                  trust_remote_code=True,\n",
    "                                  device_map='auto',\n",
    "                                  ).half()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4344dc5-c16b-47e3-998b-977193014f45",
   "metadata": {},
   "source": [
    "# prepare model for train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22aaab5a-7ad0-48eb-be9e-e64cf378dad4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T06:32:13.180510Z",
     "iopub.status.busy": "2023-07-17T06:32:13.179827Z",
     "iopub.status.idle": "2023-07-17T06:32:13.187808Z",
     "shell.execute_reply": "2023-07-17T06:32:13.187281Z",
     "shell.execute_reply.started": "2023-07-17T06:32:13.180476Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-07-17 06:32:13.183\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`\u001b[0m\n",
      "\u001b[32m2023-07-17 06:32:13.185\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mbut during inference make sure to set it back to True\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    # freeze model parameters\n",
    "    param.requires_grad = False\n",
    "\n",
    "# For backward compatibility\n",
    "model.enable_input_require_grads()\n",
    "# Enable gradient checkpoint\n",
    "model.gradient_checkpointing_enable()\n",
    "logger.info(\"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`\")\n",
    "# `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`\n",
    "model.config.use_cache = False\n",
    "# but during inference make sure to set it back to True\n",
    "logger.info(\"but during inference make sure to set it back to True\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87359fb-37f0-4bec-b152-0485edeb3947",
   "metadata": {},
   "source": [
    "# LoRa Config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a67e19a-4204-4f19-82f5-9b1971e44d7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T06:33:20.545209Z",
     "iopub.status.busy": "2023-07-17T06:33:20.544800Z",
     "iopub.status.idle": "2023-07-17T06:33:20.549922Z",
     "shell.execute_reply": "2023-07-17T06:33:20.549290Z",
     "shell.execute_reply.started": "2023-07-17T06:33:20.545183Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-07-17 06:33:20.546\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mLora_config--->LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, base_model_name_or_path=None, revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, r=12, target_modules=['query_key_value'], lora_alpha=24, lora_dropout=0.05, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "Lora_config = LoraConfig(\n",
    "    r=lora_args.r,\n",
    "    target_modules=lora_args.target_modules,\n",
    "    lora_alpha=lora_args.lora_alpha,\n",
    "    lora_dropout=lora_args.lora_dropout,\n",
    "    bias=lora_args.bias,\n",
    "    inference_mode=lora_args.inference_mode,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    layers_to_transform=lora_args.layers_to_transform,\n",
    "    layers_pattern=lora_args.layers_pattern\n",
    ")\n",
    "logger.info(f\"Lora_config--->{Lora_config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61413f34-aeac-4eaa-ba77-ec1490ff8c69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T06:33:32.399324Z",
     "iopub.status.busy": "2023-07-17T06:33:32.398762Z",
     "iopub.status.idle": "2023-07-17T06:33:40.860228Z",
     "shell.execute_reply": "2023-07-17T06:33:40.859430Z",
     "shell.execute_reply.started": "2023-07-17T06:33:32.399299Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-07-17 06:33:40.857\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mlora_model--->PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): ChatGLMForConditionalGeneration(\n",
      "      (transformer): ChatGLMModel(\n",
      "        (embedding): Embedding(\n",
      "          (word_embeddings): Embedding(65024, 4096)\n",
      "        )\n",
      "        (rotary_pos_emb): RotaryEmbedding()\n",
      "        (encoder): GLMTransformer(\n",
      "          (layers): ModuleList(\n",
      "            (0-27): 28 x GLMBlock(\n",
      "              (input_layernorm): RMSNorm()\n",
      "              (self_attention): SelfAttention(\n",
      "                (query_key_value): Linear(\n",
      "                  in_features=4096, out_features=4608, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (LoraAdapter): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (LoraAdapter): Linear(in_features=4096, out_features=12, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (LoraAdapter): Linear(in_features=12, out_features=4608, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (core_attention): CoreAttention(\n",
      "                  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (dense): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "              )\n",
      "              (post_attention_layernorm): RMSNorm()\n",
      "              (mlp): MLP(\n",
      "                (dense_h_to_4h): Linear(in_features=4096, out_features=27392, bias=False)\n",
      "                (dense_4h_to_h): Linear(in_features=13696, out_features=4096, bias=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (final_layernorm): RMSNorm()\n",
      "        )\n",
      "        (output_layer): Linear(in_features=4096, out_features=65024, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, Lora_config, lora_args.Adapter_name)\n",
    "logger.info(f\"lora_model--->{model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5439c095-a9f1-4f2c-a8a3-bf7b7b4ab747",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T06:34:08.526866Z",
     "iopub.status.busy": "2023-07-17T06:34:08.526458Z",
     "iopub.status.idle": "2023-07-17T06:34:08.533172Z",
     "shell.execute_reply": "2023-07-17T06:34:08.532628Z",
     "shell.execute_reply.started": "2023-07-17T06:34:08.526841Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-07-17 06:34:08.527\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[1mprint trainable parameters\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,924,544 || all params: 6,246,508,544 || trainable%: 0.04681885855753982\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"print trainable parameters\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac75ea0-64ed-42de-856a-da3f76fb8c45",
   "metadata": {},
   "source": [
    "# train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f39041d-b991-49f9-ac0c-148cc9e10681",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T06:34:53.360227Z",
     "iopub.status.busy": "2023-07-17T06:34:53.359808Z",
     "iopub.status.idle": "2023-07-17T06:34:53.363669Z",
     "shell.execute_reply": "2023-07-17T06:34:53.363077Z",
     "shell.execute_reply.started": "2023-07-17T06:34:53.360200Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compute model output perplexity\n",
    "def compute_metrics(loss):\n",
    "    loss_mean = loss.mean()\n",
    "    perplexity = np.exp2(loss_mean)\n",
    "    return {\"perplexity\": perplexity}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f4cb474-6ace-44b8-9b73-610ef0b9817f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T06:35:06.877492Z",
     "iopub.status.busy": "2023-07-17T06:35:06.877096Z",
     "iopub.status.idle": "2023-07-17T06:35:06.881143Z",
     "shell.execute_reply": "2023-07-17T06:35:06.880374Z",
     "shell.execute_reply.started": "2023-07-17T06:35:06.877467Z"
    }
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        model=model,\n",
    "        pad_to_multiple_of=8,\n",
    "        padding=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f8acd87-a485-4945-9364-dd49e62c6094",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T06:35:18.378082Z",
     "iopub.status.busy": "2023-07-17T06:35:18.377548Z",
     "iopub.status.idle": "2023-07-17T06:35:18.388864Z",
     "shell.execute_reply": "2023-07-17T06:35:18.388355Z",
     "shell.execute_reply.started": "2023-07-17T06:35:18.378040Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.\n"
     ]
    }
   ],
   "source": [
    "trainer = LoraTrainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=datasets_train,\n",
    "    eval_dataset=datasets_valid,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a91497b8-0128-429e-8614-a16375acdd78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T06:35:51.094587Z",
     "iopub.status.busy": "2023-07-17T06:35:51.094181Z",
     "iopub.status.idle": "2023-07-17T06:35:51.098523Z",
     "shell.execute_reply": "2023-07-17T06:35:51.097985Z",
     "shell.execute_reply.started": "2023-07-17T06:35:51.094558Z"
    }
   },
   "outputs": [],
   "source": [
    "resume_from_checkpoint = finetune_args.resume_from_checkpoint\n",
    "if resume_from_checkpoint is not None:\n",
    "    if os.path.exists(resume_from_checkpoint):\n",
    "        logger.info(f'Restarting from {resume_from_checkpoint}')\n",
    "        model.load_adapter(resume_from_checkpoint, lora_args.Adapter_name, subfolder=lora_args.Adapter_name)\n",
    "    else:\n",
    "        raise Exception(f'{resume_from_checkpoint} is not a correct path!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "667f2d91-1586-44f4-9d32-2aee4f19e42f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T06:36:01.004478Z",
     "iopub.status.busy": "2023-07-17T06:36:01.003737Z",
     "iopub.status.idle": "2023-07-17T06:36:01.008081Z",
     "shell.execute_reply": "2023-07-17T06:36:01.007551Z",
     "shell.execute_reply.started": "2023-07-17T06:36:01.004454Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-07-17 06:36:01.005\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[1mstart training from None\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"start training from {resume_from_checkpoint}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad075ec-ac27-4d09-b466-45053695d1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=resume_from_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed43224-3ff9-426c-bcb2-6103a96d01ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"training finished, save model to {finetune_args.final_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31ae882-e787-46dd-bc8d-beb519232f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(finetune_args.final_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a909a9-247b-42c4-b859-3b373153f8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(trainer.args, os.path.join(finetune_args.final_model_path, \"training_args.bin\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
